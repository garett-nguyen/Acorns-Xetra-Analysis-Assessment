package com.gnguyen.Util;
import java.time.LocalDate;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import java.time.YearMonth;
import java.time.LocalDate;
import java.time.MonthDay;

public class StockVolume {
	public static void createDataframes(JavaSparkContext context, SparkSession session) {
		
		//Creates LocalDate range January 1st of 2018 to December 31st of 2018.
		
		String s3bucketname = "s3a://deutsche-boerse-xetra-pds/";	
		LocalDate start = LocalDate.parse("2018-01-01");
		LocalDate end = LocalDate.parse("2018-12-31");
		LocalDate next = start.minusDays(1);
		
		//Uses file names to access s3 csv files, then creates RDDs of each unique file. 
		
		while ((next = next.plusDays(1)).isBefore(end.plusDays(1))) {
				
			for (int j = 1; j < 24; j++) {
			

				
			}
			

		}
	}
}