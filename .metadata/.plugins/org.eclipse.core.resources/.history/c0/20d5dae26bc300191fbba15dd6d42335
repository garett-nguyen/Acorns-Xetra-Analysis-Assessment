package com.gnguyen.Util;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import java.time.YearMonth;
import java.time.LocalDate;
import java.time.MonthDay;

public class StockVolume {
	public static void calc(JavaSparkContext context, SparkSession session) {
		
				//Creates LocalDate range January 1st of 2018 to December 31st of 2018.
				
				String s3bucketname = "s3a://deutsche-boerse-xetra-pds/";	
				LocalDate start = LocalDate.parse("2018-01-01");
				LocalDate end = LocalDate.parse("2018-12-31");
				LocalDate next = start.minusDays(1);
				
				//Creates 24 file names per day of the year, 365 days in 2018.
				//Uses file names to access s3 csv files, then creates RDDs of each unique file. 
				
				while ((next = next.plusDays(1)).isBefore(end.plusDays(1))) {
						for (int j = 0; j < 24; j++) {
						
						String s3filename = s3bucketname + next + "/" + next + 
								"_BINS_XETRA" + String.format("%02d", j) + ".csv";
						System.out.println(s3filename);
						try {
							JavaRDD<String> distFile = context.textFile(s3filename);
							Dataset<Row> data = session.createDataFrame(distFile, String.class);
							
							
							
						}
						catch (Exception e) {
						}
					}
				}

				

		
		
	}
	
}
